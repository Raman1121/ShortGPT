{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from llama import Llama\n",
    "\n",
    "from short_llama import ShortLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivaen\\anaconda3\\envs\\shortgpt\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for pg19 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/pg19\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"pg19\", split=\"validation\")  # authors sample 10,000 texts to compute block influences\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator(device=\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and Wrap Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivaen\\anaconda3\\envs\\shortgpt\\lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 12.17 seconds\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 1024  # authors use a context width of 1024\n",
    "llama = Llama.build(\n",
    "    ckpt_dir=\"../llama/llama-2-7b\",\n",
    "    tokenizer_path=\"../llama/tokenizer.model\",\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    max_batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x TransformerBlock(\n",
       "    (attention): Attention(\n",
       "      (wq): ColumnParallelLinear()\n",
       "      (wk): ColumnParallelLinear()\n",
       "      (wv): ColumnParallelLinear()\n",
       "      (wo): RowParallelLinear()\n",
       "    )\n",
       "    (feed_forward): FeedForward(\n",
       "      (w1): ColumnParallelLinear()\n",
       "      (w2): RowParallelLinear()\n",
       "      (w3): ColumnParallelLinear()\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ffn_norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama = ShortLlama(llama=llama)\n",
    "\n",
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': '1960s-70s era pop music. I grew up listening to the radio'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample generation\n",
    "short_llama.llama.text_completion(\n",
    "    prompts=[\"I am an avid fan of \"],\n",
    "    max_gen_len=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832e0ddbe80b4106982edb07de4fa9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    prompts = batch['text']\n",
    "\n",
    "    prompt_tokens = [short_llama.llama.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "\n",
    "    # authors use a sliding window of size 1024 with a shift of 256\n",
    "    for start in range(0, max_prompt_len, 256):\n",
    "\n",
    "        inputs = [p[start:start+MAX_SEQ_LEN] for p in prompt_tokens if len(p) > start]\n",
    "\n",
    "        short_llama.eval_importance(\n",
    "            prompt_tokens=inputs,\n",
    "            max_gen_len=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16717843.43359375,\n",
       " 10423418.44140625,\n",
       " 6518133.3359375,\n",
       " 6328185.017578125,\n",
       " 7037034.49609375,\n",
       " 6307392.001953125,\n",
       " 6125241.50390625,\n",
       " 5712124.599609375,\n",
       " 5348248.4765625,\n",
       " 5091788.0625,\n",
       " 4765901.00390625,\n",
       " 4389966.291015625,\n",
       " 4292717.021484375,\n",
       " 4361633.55859375,\n",
       " 4291800.3046875,\n",
       " 4252424.794921875,\n",
       " 4361357.048828125,\n",
       " 3372381.509765625,\n",
       " 3048071.146484375,\n",
       " 2540082.32421875,\n",
       " 2737189.0546875,\n",
       " 1909176.11328125,\n",
       " 1889121.580078125,\n",
       " 1560965.88671875,\n",
       " 1487861.056640625,\n",
       " 1465746.361328125,\n",
       " 1490804.53125,\n",
       " 1466835.6328125,\n",
       " 1524585.98828125,\n",
       " 1542287.908203125,\n",
       " 2607044.50390625,\n",
       " 11649695.109375]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unimportant layers\n",
    "\n",
    "Layers removed when using pg19 val set: [25, 27, 24, 26, 28, 29, 23, 22, 21]\n",
    "\n",
    "Note: Different order than paper but same 9 least important layers -> [27, 26, 25, 28, 24, 29, 23, 21, 22]\n",
    "\n",
    "Additionally, authors mention that the layer order is quite nuanced and can vary with different datasets. However, relative order suggests similar importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 24, 26, 28, 29, 23, 22, 21]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.remove_layers(num_layers=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-22): 23 x TransformerBlock(\n",
       "    (attention): Attention(\n",
       "      (wq): ColumnParallelLinear()\n",
       "      (wk): ColumnParallelLinear()\n",
       "      (wv): ColumnParallelLinear()\n",
       "      (wo): RowParallelLinear()\n",
       "    )\n",
       "    (feed_forward): FeedForward(\n",
       "      (w1): ColumnParallelLinear()\n",
       "      (w2): RowParallelLinear()\n",
       "      (w3): ColumnParallelLinear()\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ffn_norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the paper states: \\\n",
    "    - \"Our experiments reveal that the effect of layer removal is significantly more pronounced on generative\n",
    "        tasks compared to multiple-choice tasks. On benchmarks such as GSM8K (Cobbe et al., 2021) and\n",
    "        HumanEval (Chen et al., 2021), removing 25% of the layers often leads to a severe performance\n",
    "        drop, with scores approaching zero.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': 'Đo n Khơ 20th Century. Hinweis: In = ,t and lồ'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.llama.text_completion(\n",
    "    prompts=[\"I am an avid fan of \"],\n",
    "    max_gen_len=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
